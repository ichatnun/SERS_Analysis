{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install webcolors\n",
    "# !{sys.executable} -m pip install plotly==5.7.0\n",
    "# !{sys.executable} -m pip install --upgrade nbformat\n",
    "# !{sys.executable} -m pip install tune-sklearn ray[tune]\n",
    "# !{sys.executable} -m pip install packaging\n",
    "# !{sys.executable} -m pip install kaleido==5.7.0\n",
    "# !{sys.executable} -m pip install ffmpeg-python\n",
    "# !{sys.executable} -m pip install BaselineRemoval\n",
    "# !conda install --yes --prefix {sys.prefix} catboost\n",
    "# !conda install --yes --prefix {sys.prefix} -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import necessary packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pdb, itertools, os, time, sys, re, random, pickle, shutil\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from config import current_config as config\n",
    "from models.performAnalysis import performAnalysis\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DejaVu Sans\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Check the validity of the paremeters\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_classif_methods = ['pc-lda', \n",
    "                             'rf', 'pc-rf', 'lda-rf', \n",
    "                             'svm','pc-svm','lda-svm',\n",
    "                             'catboost','pc-catboost','lda-catboost',\n",
    "                             'lgbm','pc-lgbm','lda-lgbm',\n",
    "                             'logistic-reg','pc-logistic-reg','lda-logistic-reg']\n",
    "                             \n",
    "supported_ML_methods = supported_classif_methods + ['pca']\n",
    "\n",
    "\n",
    "ML_method_list = config.ML_method_list\n",
    "# Get rid of unsupported ML methods\n",
    "for curr_ML_method in ML_method_list:\n",
    "    if not (curr_ML_method in supported_ML_methods):\n",
    "        ML_method_list.remove(curr_ML_method)\n",
    "        print(curr_ML_method,'is currently unsupported')\n",
    "print('The ML methods that will be used include', ML_method_list)\n",
    "\n",
    "if not (config.data_split_method in ['intra','inter','by-folders']):\n",
    "    print('Invalid data_split_method.')\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Prepare some information\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a list of random seeds to perform experiments\n",
    "if config.data_split_method == 'intra':\n",
    "    random_seed_list = np.random.randint(config.num_repetitions_intra*10, size=config.num_repetitions_intra)\n",
    "\n",
    "# Define the base directory\n",
    "cwd = os.getcwd()\n",
    "if config.data_split_method in ['intra','inter']:\n",
    "    data_base_dir = os.path.join(cwd,'data_auto_split')\n",
    "else:\n",
    "    data_base_dir = os.path.join(cwd,'data_manual_split')\n",
    "        \n",
    "exp_base_dir = os.path.join(data_base_dir,config.experiment_name)\n",
    "\n",
    "# Define a list of experiments\n",
    "if os.path.exists(exp_base_dir):\n",
    "    experiment_name_list = [exp_base_dir]\n",
    "else:\n",
    "    print('No such experiment. These are all existing experiments: ')\n",
    "    experiment_name_list = create_dir_list_no_hidden_dir(data_base_dir)\n",
    "    for curr_exp_name in experiment_name_list:\n",
    "        print(curr_exp_name)\n",
    "    user_response = input('Input [c] if you want to run these experiments: ')\n",
    "    if user_response != 'c':\n",
    "        print(\"You have decided to quit.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"You have decided to run the experiments listed above.\")\n",
    "\n",
    "num_exps = len(experiment_name_list)\n",
    "num_ML_methods = len(ML_method_list)\n",
    "\n",
    "# Count the number of classification methods. Intentionally didn't implement the set approach \n",
    "# to make sure of the consistency between the orders of the lists.\n",
    "ML_classif_method_list = []\n",
    "for curr_ML_method in ML_method_list:\n",
    "    if curr_ML_method in supported_classif_methods:\n",
    "        ML_classif_method_list.append(curr_ML_method)\n",
    "num_classif_methods = len(ML_classif_method_list)\n",
    "\n",
    "# Generate color list for classification bar plots\n",
    "color_list_for_bar_plots = []\n",
    "for idx_ML_method in range(num_classif_methods):\n",
    "    color_list_for_bar_plots.append(rgb_to_hex((random.randint(0,255), random.randint(0,255), random.randint(0,255))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_exp, curr_exp_path in enumerate(experiment_name_list):\n",
    "    \n",
    "    # Prepare a directory for saving the results\n",
    "    curr_exp_name = os.path.basename(curr_exp_path)\n",
    "    results_path = os.path.join(cwd,config.results_folder_name, curr_exp_name, config.data_split_method)\n",
    "\n",
    "    if config.REMOVE_BASELINE:\n",
    "        results_path += '_baseline_removed'\n",
    "\n",
    "    if os.path.exists(results_path):\n",
    "        shutil.rmtree(results_path)\n",
    "    \n",
    "    os.makedirs(results_path)\n",
    "    \n",
    "    # Copy the config file\n",
    "    shutil.copy2('./config/current_config.py', os.path.join(results_path))\n",
    "    \n",
    "        \n",
    "    print('***********************************************************')\n",
    "    print('Begin experiment ', idx_exp+1, '/', num_exps,':',curr_exp_name)\n",
    "    print('***********************************************************')\n",
    "    \n",
    "    # Extract classes\n",
    "    if config.data_split_method in ['intra','inter']:\n",
    "        class_list = get_class_names(curr_exp_path)\n",
    "        \n",
    "    elif config.data_split_method in ['by-folders']:\n",
    "        class_list_train = get_class_names(os.path.join(curr_exp_path,'train'))\n",
    "        class_list_test = get_class_names(os.path.join(curr_exp_path,'test'))\n",
    "        \n",
    "        if set(class_list_train) == set(class_list_test):\n",
    "            class_list = class_list_test\n",
    "        else:\n",
    "            print('Training and test class lists are not the same.')\n",
    "            sys.exit(1)\n",
    "        \n",
    "    print('Classes = ', class_list)\n",
    "            \n",
    "    # Generate a color list for each class\n",
    "    colors_list,color_dict = createColorList(config.color_dict,class_list)\n",
    "    print('and their corresponding colors: ', colors_list)\n",
    "    \n",
    "    ########## Load data ##########\n",
    "    \n",
    "    # If the user has provided the raman shift file, load it.\n",
    "    ramanshifts_filepath = os.path.join(curr_exp_path,'raman_shifts.csv')\n",
    "    if os.path.exists(ramanshifts_filepath):\n",
    "        df = pd.read_csv(ramanshifts_filepath)\n",
    "        desired_ramanshifts = df['shifts'].values\n",
    "        del df\n",
    "        print('Use the raman shifts specified in', ramanshifts_filepath)\n",
    "        shutil.copy2(ramanshifts_filepath, os.path.join(results_path))\n",
    "\n",
    "    else:\n",
    "        # Determine common wavenumbers\n",
    "        desired_ramanshifts = getCommonWavenumbers(curr_exp_path, class_list)\n",
    "            \n",
    "        df_out = pd.DataFrame(desired_ramanshifts,columns=['shifts'])\n",
    "        df_out.to_csv(os.path.join(results_path,'raman_shifts.csv'),index=False,index_label='shifts')\n",
    "        del df_out\n",
    "\n",
    "\n",
    "    # Load the data from the 'intra' and 'inter' cases\n",
    "    if config.data_split_method in ['intra','inter']:\n",
    "        \n",
    "        # Extract the spectra and interpolate the wavenumbers to align with 'common_wavenumbers'\n",
    "        spectra, _ = extractAllSpectra(curr_exp_path, \n",
    "                                       class_list,\n",
    "                                       desired_wavenumbers=desired_ramanshifts,\n",
    "                                       REMOVE_BASELINE=config.REMOVE_BASELINE,\n",
    "                                       OVERWRITE_PRELOADED_DATA=config.OVERWRITE_PRELOADED_DATA)\n",
    "        del desired_ramanshifts\n",
    "\n",
    "        ########## Define the data-splitting parameters ##########\n",
    "        if config.data_split_method in ['intra']:\n",
    "            num_repetitions = config.num_repetitions_intra\n",
    "            fraction_test =  config.fraction_test_intra\n",
    "\n",
    "        elif config.data_split_method in ['inter']:\n",
    "\n",
    "            # Figure out the minimum number of files in a class for the current experiment\n",
    "            min_num_files_per_class = 100000\n",
    "            for curr_class in class_list:\n",
    "                curr_num_files_per_class = len(glob.glob(os.path.join(curr_exp_path,curr_class,'*.txt')))\n",
    "                if min_num_files_per_class > curr_num_files_per_class:\n",
    "                    min_num_files_per_class = curr_num_files_per_class\n",
    "\n",
    "            num_repetitions = min_num_files_per_class\n",
    "            fraction_test = 1.0/min_num_files_per_class\n",
    "            \n",
    "    elif config.data_split_method in ['by-folders']:\n",
    "        num_repetitions = 1\n",
    "        spectra_training, _ = extractAllSpectra(os.path.join(curr_exp_path,'train'), \n",
    "                                                class_list,\n",
    "                                                desired_wavenumbers=desired_ramanshifts,\n",
    "                                                REMOVE_BASELINE=config.REMOVE_BASELINE,\n",
    "                                                OVERWRITE_PRELOADED_DATA=config.OVERWRITE_PRELOADED_DATA)\n",
    "        spectra_test, _ = extractAllSpectra(os.path.join(curr_exp_path,'test'), \n",
    "                                            class_list,\n",
    "                                            desired_wavenumbers=desired_ramanshifts,\n",
    "                                            REMOVE_BASELINE=config.REMOVE_BASELINE,\n",
    "                                            OVERWRITE_PRELOADED_DATA=config.OVERWRITE_PRELOADED_DATA)\n",
    "        \n",
    "    \n",
    "    acc_all_reps_all_methods = np.zeros((num_repetitions,num_classif_methods))\n",
    "    precision_all_reps_all_methods = np.zeros((num_repetitions,num_classif_methods))\n",
    "    recall_all_reps_all_methods = np.zeros((num_repetitions,num_classif_methods))\n",
    "    f1_all_reps_all_methods = np.zeros((num_repetitions,num_classif_methods))\n",
    "    \n",
    "    for idx_rep in range(num_repetitions):\n",
    "        \n",
    "        print('Rep ', idx_rep+1, '/', num_repetitions,'\\n')            \n",
    "\n",
    "        ########## Perform training/test data splitting ##########\n",
    "        if config.data_split_method in ['intra','inter']:\n",
    "            params_data_splitting = {'data_split_method':config.data_split_method,\n",
    "                                     'spectra':spectra,\n",
    "                                     'class_list':class_list,\n",
    "                                     'fraction_test':fraction_test}\n",
    "\n",
    "            if config.data_split_method in ['intra']:\n",
    "                params_data_splitting.update({'random_seed': random_seed_list[idx_rep]})\n",
    "                spectra_training, spectra_test, labels_training, labels_test = trainTestSplitting(**params_data_splitting)\n",
    "\n",
    "            elif config.data_split_method in ['inter']:\n",
    "                params_data_splitting.update({'idx_target_file_for_inter_test': idx_rep})\n",
    "                spectra_training, spectra_test, labels_training, labels_test = trainTestSplitting(**params_data_splitting)\n",
    "        \n",
    "        elif config.data_split_method in ['by-folders']:\n",
    "            spectra_training, labels_training = loadSpectraAndLabels(spectra_training, class_list)\n",
    "            spectra_test, labels_test = loadSpectraAndLabels(spectra_test, class_list)\n",
    "            \n",
    "    \n",
    "        print('# of training spectra = ', spectra_training.shape[0])\n",
    "        print('# of test spectra = ', spectra_test.shape[0])\n",
    "\n",
    "        ########## Apply each ML method ##########\n",
    "        count_classif_method = 0\n",
    "        params = {'class_list':class_list,\n",
    "                 'x_train':spectra_training,\n",
    "                 'y_train':labels_training,\n",
    "                 'x_test':spectra_test,\n",
    "                 'y_test':labels_test,\n",
    "                 'color_dict':color_dict}\n",
    "        \n",
    "        for idx_ML_method, curr_ML_method in enumerate(ML_method_list):\n",
    "            \n",
    "            print('\\n\\tML method ', idx_ML_method+1, '/',num_ML_methods,':',curr_ML_method)\n",
    "            \n",
    "            # Prepare the folder to save the results\n",
    "            temp_base_results_path = os.path.join(results_path,\n",
    "                       'rep'+str(idx_rep+1),\n",
    "                       curr_ML_method)\n",
    "            \n",
    "            if not os.path.exists(temp_base_results_path):\n",
    "                os.makedirs(temp_base_results_path)\n",
    "                \n",
    "            params.update({'results_path':temp_base_results_path})\n",
    "                    \n",
    "            \n",
    "            # Apply the current ML model\n",
    "            if curr_ML_method in supported_classif_methods:\n",
    "                \n",
    "                eval_metrics_dict = performAnalysis(method_name=curr_ML_method, params=params)\n",
    "                \n",
    "                # Update the evaluation metric matrices\n",
    "                acc_all_reps_all_methods[idx_rep,count_classif_method] = eval_metrics_dict['accuracy']*100\n",
    "                precision_all_reps_all_methods[idx_rep,count_classif_method] = eval_metrics_dict['weighted avg']['precision']\n",
    "                recall_all_reps_all_methods[idx_rep,count_classif_method] = eval_metrics_dict['weighted avg']['recall']\n",
    "                f1_all_reps_all_methods[idx_rep,count_classif_method] = eval_metrics_dict['weighted avg']['f1-score']\n",
    "                \n",
    "                count_classif_method += 1\n",
    "                \n",
    "            else: \n",
    "                performAnalysis(method_name=curr_ML_method, params=params)\n",
    "            \n",
    "    # Save the evaluation metrics: accuracy, precision, recall, f1-score\n",
    "    saveSingleEvaluationMetrics_AllMethods(acc_all_reps_all_methods,\n",
    "                                       'accuracy',\n",
    "                                       num_repetitions,\n",
    "                                       ML_classif_method_list,\n",
    "                                       color_list_for_bar_plots,\n",
    "                                       results_path)\n",
    "\n",
    "    saveSingleEvaluationMetrics_AllMethods(precision_all_reps_all_methods,\n",
    "                                       'precision',\n",
    "                                       num_repetitions,\n",
    "                                       ML_classif_method_list,\n",
    "                                       color_list_for_bar_plots,\n",
    "                                       results_path) \n",
    "\n",
    "    saveSingleEvaluationMetrics_AllMethods(recall_all_reps_all_methods,\n",
    "                                       'recall',\n",
    "                                       num_repetitions,\n",
    "                                       ML_classif_method_list,\n",
    "                                       color_list_for_bar_plots,\n",
    "                                       results_path)       \n",
    "                                                                      \n",
    "    saveSingleEvaluationMetrics_AllMethods(f1_all_reps_all_methods,\n",
    "                                       'f1',\n",
    "                                       num_repetitions,\n",
    "                                       ML_classif_method_list,\n",
    "                                       color_list_for_bar_plots,\n",
    "                                       results_path) \n",
    "\n",
    "print('Done')\n",
    "print('***********************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sers-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
